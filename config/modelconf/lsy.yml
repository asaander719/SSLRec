optimizer:
  name: adam
  lr: 1.0e-3
  weight_decay: 0 #0.0001 #0

train:
  epoch: 200
  batch_size: 3 #256 #512
  save_model: true
  log_loss: false
  test_step: 1
  reproducible: true
  seed: 2023
  pretrain_path: "/home/asaliao/SSLRec/checkpoint/lsy/tune/lsy-reddit-.pth" #"/home/asaliao/SSLRec/checkpoint/lsy/lsy-reddit-1710605539.pth"

test:
  metrics: [recall, ndcg, mrr]
  k: [5, 10, 20]
  batch_size: 3 #256 #512

data:
  type: sequential
  name: beauty #ml-20m

model:
  name: lsy
  dropout_rate: 0.1
  n_layers: 2
  embedding_size: 64
  mask_ratio: 0.2
  loss_type: 'CE'
  n_heads: 2
  max_seq_len: 50
  scales: [1, 2, 4] #[10, 4, 20]
  hidden_act: "gelu" #"relu", "swish", "tanh", "sigmoid" 
  hidden_size: 64
  inner_size: 256
  hidden_dropout_prob: 0.5
  attn_dropout_prob: 0.5
  feed: True
  lmd: 0.1
  tau: 1
  cl_weight: 0.1
  replace_ratio: 0.5
  sim_method: "cos"
  with_contri: False
  cont_method: mlp #"att"
  aug_ratio: 0.3
  aug_k: 1 #2
  aug_with_maxself: False
  # enable_hg: 1  
  # enable_ms: 1  
  # customized_eval: 0  # 1,
  # sim_threshold: 0.5
  # graph_estr: 1
  # loss_hg_w: 0.8
  # enable_sim: 1
  # global_hg: True
  # att_aggre: False # False is better
  # li_aggre_w: 0.9
  # temperature: 5
  # hyper_len: 6

tune:
  enable: True
  hyperparameters: []
  layer_num: []
  reg_weight: []
  lmb: [0.05, 0.1, 0.2]
  tau: [0.5, 0.7, 0.9, 1]
  cl_weight: [0.1, 0.2, 0.5, 1, 2]
  replace_ratio: [0.1, 0.2, 0.3, 0.5, 0.8, 1]
  sim_method: ["mm", "cos", "gate"]

